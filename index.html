<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Le pouvoir du xaringan : créez des présentations élégantes et reproductibles avec RMarkdown</title>
    <meta charset="utf-8" />
    <meta name="author" content="Francis Duval" />
    <meta name="date" content="2024-02-19" />
    <script src="libs/header-attrs-2.23/header-attrs.js"></script>
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <link rel="stylesheet" href="theme_cara.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title-slide
background-image: url(images/logo_uqam.png), url(images/background.jpg)
background-size: 20%, cover
background-position: 95% 95%, center

.titre-page-titre[Modélisation des sinistres en assurance automobile avec l'utilisation de données télématiques : Approches d'apprentissage automatique en classification et régression de comptage]
&lt;br /&gt;
.sous-titre-page-titre[Soutenance de thèse]
&lt;br /&gt;
***
&lt;br /&gt;
.sous-sous-titre-page-titre[.mon-style-bleu[par] Francis Duval &lt;br&gt; .mon-style-bleu[le] 26 février 2024]

???

- Bonjour tout le monde! Tout d'abord, merci d'être venus assister à ma soutenance de thèse.
- Ma thèse s'intitule « Modélisation des sinistres en assurance automobile avec l'utilisation de données télématiques : Approches d'apprentissage automatique en classification et régression de comptage ».

---

# Contenu
&lt;br&gt;
.left[
  .bleu-gros[1) Introduction] &lt;br&gt; &lt;br&gt; &lt;br&gt;
  .bleu-gros[2) Chapitre 1] &lt;br&gt; **How Much Telematics Information Do Insurers Need for Claim Classification?** &lt;br&gt; &lt;br&gt;
  .bleu-gros[3) Chapitre 2] &lt;br&gt; **Enhancing Claim Classification with Feature Extraction From Anomaly-Detection-Derived Routine and Peculiarity Profiles** &lt;br&gt; &lt;br&gt;
  .bleu-gros[4) Chapitre 3] &lt;br&gt; **Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data** &lt;br&gt; &lt;br&gt;
  .bleu-gros[5) Conclusion]
]

???

- Voici un aperçu du contenu de la présentation.
- Dans la Section 1, l'introduction, je vais vous présenter quelques principes de base qui ont servi de piliers dans ma recherche. Je vais aussi vous présenter les données sur lesquelles j'ai travaillé.
- C'est une thèse par articles, c'est-à-dire qu'elle consiste en 3 articles distincts. Aux Sections 2, 3 et 4, je vous présente chacun de ces 3 articles.
- Finalement, on conclut à la Section 5.

---

# Introduction

## Assurance basée sur l'usage

- Tarification en assurance automobile traditionellement faite avec un ensemble limité de **caractéristiques du risque**.
- Avec l'avènement de la technologie **télématique**, possibilité de collecter des données sur la conduite des assurés.

.center[
&lt;img src="images/sep_car.png" width="4%" style="display: block; margin: auto;" /&gt;
]
.titre-bloc.bleu[
De nombreux bénéfices!
]
.bloc.titre.bleu[
- Incitatif pour une conduite plus parcimonieuse et sécuritaire.
  - Amélioration de la sécurité des routes.
  - Réduction des gaz à effet de serre.
- Tarification plus précise et équitable basée sur le comportement de conduite réel de chaque individu.
- Substituts à des attributs considérés éthiquement sensibles tels que le genre.
]

???

- Ma thèse vise à améliorer la modélisation des sinistres en assurance automobile, plus précisément en assurance basée sur l'usage.
- L'assurance basée sur l'usage est un modèle d'assurance où les primes sont calculées en fonction de données collectées sur l'assuré.
- En assurance automobile, ces données, qu'on appelle « données télématiques », sont collectées à l'aide d'un dispositif installé dans le véhicule ou d'une application mobile.
- Ces données peuvent inclure la distance parcourue par l'assuré, les heures de conduite, la localisation, les freinages brusques, et bien d'autres encore.
- Traditionnellement, la tarification est faite en ne regardant pas l'utilisation du véhicule. On tarifie avec un ensemble de caratéristiques du risque comme l'âge et le genre de l'assuré, l'âge du véhicule, la région et plusieurs autres.

- Tarifer selon l'utilisation offre de nombreux avantages pour tout le monde. 
- Avant tout, ça constitue un incitatif à mieux et à moins conduire pour les assurés, ce qui permet d'améliorer la sécurité des routes et de diminuer la pollution. Donc on a une diminution de l'aléa moral, un effet qui est omniprésent en assurance.
- Pour un assureur, ça permet de mieux segmenter la prime, et donc d'aller chercher un avantage compétitif.
- Les données télématiques peuvent également servir de substitus à des attributs sensibles ou des attributs qui sont interdits d'utilisation par la loi. Par exemple, dans certaines juridictions, on ne peut pas tarifer selon le genre de l'assuré.
- Je n'ai pas parlé des inconvénients ici, mais c'est certain qu'utilisées de manière intelligente et responsable, les bénéfices qu'on peut tirer des données télématiques surpassent les inconvénients, et c'est pour ça que c'est important de faire de la recherche là-dessus.   

---

background-image: url(images/machine_learning_picto.png)
background-size: 10%
background-position: 95% 20%

`$$\DeclareMathOperator*{\argmin}{argmin}$$`
.vspace-neg[]

# Introduction

## Apprentissage supervisé &amp;#x2014; Idée générale

.vspace-neg2[]

- Requiert un ensemble d'**exemples étiquetés**
`\begin{align*}
  \{(\boldsymbol{x}_i, y_i)\}_{i=1}^n, \quad \text{où}\quad \boldsymbol{x}_i \in \mathcal{X},\quad y \in \mathcal{Y}
\end{align*}`

???

- Le principal outil utilisé dans la thèse est l'apprentissage supervisé.
- Pour être entrainé, un algorithme d'apprentissage supervisé requiert un jeu de données d'exemples étiquetés.
- Donc pour chaque exemple i, on a un vecteur de prédicteurs x qui est associé à une étiquette, ou variable réponse y.
- Par exemple, la variable d'intérêt y pourrait être le coût de réclamation et x, un vecteur de caractéristiques du risque.

--

- But: modéliser la **variable réponse** `\(y\)` à l'aide des **prédicteurs** `\(\boldsymbol{x}\)`. On cherche une fonction `\(h: \mathcal{X} \rightarrow \mathcal{A}\)`, où `\(\mathcal{A}\)` est l'ensemble des prédictions possibles, telle que `\(h(\boldsymbol{x})\)` est la plus « proche » possible de `\(y\)`.

???

- y, c'est notre variable d'intérêt qu'on veut modéliser avec l'aide de notre vecteur x.
- Donc en fait, on cherche une fonction de x qui va nous donner une valeur la plus proche possible de y.

--

- Pour définir « proche », on introduit une **fonction de perte** `\(\ell: \mathcal{A} \times \mathcal{Y} \rightarrow \mathbb{R}^+\)`.
- `\(\ell[h(\boldsymbol{x}), y]\)` mesure la « distance » entre la prédiction `\(h(\boldsymbol{x})\)` et la réponse `\(y\)`.

???

- Pour définir « proche », on introduit une fonction de perte `\(\ell\)` qui va pour but de mesurer la « distance » entre la prédiction h(x) et la réponse y.

--

- Pour trouver une bonne fonction `\(h^*\)`, on va (essayer de) minimiser le **risque empirique**, c'est-à-dire la perte moyenne sur l'ensemble d'exemples:
`\begin{align*}
  h^* = \argmin_{h} \frac{1}{n}\sum_{i=1}^n \ell[h(\boldsymbol{x}_i), y_i]. 
\end{align*}`

???

- Et ensuite, pour trouver notre fonction h, on va habituellement essayer de minimiser ce qu'on appelle le « risque empirique » sur nos exemples d'entrainement.
- Le risque empirique, c'est simplement la somme (ou la moyenne) de la fonction de perte évaluée sur chacun des exemples.

--

- Plusieurs algorithmes d'apprentissage supervisés ont été développés pour résoudre ce problème d'optimisation (GLMs, arbres de décision, forêts aléatoires, boosting, réseaux de neurones, régularisation, etc.)

???

- Pour trouver une bonne fonction h, plusieurs algorithmes ont été développés.
- Ces algorithmes d'apprentissage nous définissent en fait un espace de recherche pour la fonction h ainsi qu'une recette pour trouver la fonction optimale (ou une bonne fonction) dans cet espace.

---

# Introduction

## Apprentissage supervisé &amp;#8212; Compromis biais-variance

**Décomposition de l'erreur**
`\begin{align*}
  \text{Erreur} = \text{Biais}^2 + \text{Variance} + \text{Erreur irréductible}
\end{align*}`

&lt;img src="images/biais_variance.png" width="90%" style="display: block; margin: auto;" /&gt;

- Le compromis biais-variance dit que l'on peut « troquer » du biais contre de la variance, et vice-versa.

???

- Un concept très important en apprentissage supervisé est le compromis biais-variance, qui est intimement lié aux concepts de sous-ajustement et surajustement.
- Donc tout d'abord, on peut décomposer l'erreur en 3 parties: le biais au carré, la variance et l'erreur irréductible.
- L'erreur irréductible, c'est l'erreur qu'on ferait même si on avait le meilleur modèle possible, c'est-à-dire si on savait la fonction qui a généré les données. On a pas de contrôle là-dessus.
- Par contre, on a le contrôle sur le biais et la variance, et donc si on veut diminuer l'erreur, on peut diminuer un des 2.
- Et ce que le compromis biais-variance dit en fait, c'est que si l'un augmente, alors l'autre va systématiquement diminuer, et vice-versa.
- L'idée en apprentissage supervisé est de trouver un bon compromis, c'est-à-dire un compromis qui va minimiser l'erreur.

- Donc ici à gauche, on a un modèle qui a une variance faible mais un biais élevé. On dit qu'il y a sous-ajustement. On aurait intérêt à augmenter la variance pour diminuer le biais. Et bien sûr, on espère que la diminution en biais va plus que compenser pour l'augmentation de la variance.
- À droite, on a un modèle qui a un biais élevé mais une variance faible. On dit qu'il y a surajustement. On aurait intérêt à augmenter le biais pour diminuer la variance.
- Au milieu, c'est ce qu'on recherche, c'est-à-dire un bon compromis entre biais et variance.

---

# Introduction

## Apprentissage supervisé &amp;#8212; Calibration des hyperparamètres

- Paramètres qui **ne** sont **pas** optimisés directement par l'algorithme.
- On essaie habituellement plusieurs combinaisons d'hyperparamètres et on prend celle menant à la meilleure performance **hors-échantillon**.
  - Besoin d'un **ensemble de validation** ou de **validation croisée**.
  
&lt;img src="images/tuning.png" width="75%" style="display: block; margin: auto;" /&gt;

???

- Souvent, en apprentissage supervisé, on doit faire la calibration des hyperparamètres.
- Un hyperparamètre, c'est simplement un paramètre qui n'est pas entrainé (ou optimisé) directement par l'algorithme.
- Ces hyperparamètres doivent donc être optimisés par le modélisateur.
- Il existe plusieurs algorithmes pour faire la calibration des hyperparamètres, comme l'optimisation Bayesienne, mais une manière simple de faire (et souvent suffisante) est de se faire une grille et d'essayer toutes les combinaisons de cette grille. On choisit alors la combinaison menant à la meilleure performance sur des exemples hors-échantillon.

---

# Introduction

## Apprentissage supervisé &amp;#8212; Estimation de l'erreur de généralisation

- L'erreur calculée sur l'ensemble de validation (ou par validation croisée) **n**'est **pas** un bon estimé de l'erreur de généralisation.
  - En effet, lors du processus de calibration des hyperparamètres, il y a « **fuite d'information** » de l'ensemble de validation vers l'ensemble d'entrainement.
- Il est donc crucial de garder un ensemble test complètement indépendant.

&lt;img src="images/separation.png" width="55%" style="display: block; margin: auto;" /&gt;

---

.vspace-neg2[]

# Introduction

## Données &amp;#8212; Variables de tarification traditionnelles

.my-one-page-font[
| Nom                      | Description                                 | Type         |
|--------------------------|---------------------------------------------|--------------|
| `expo`                   | Durée du contrat                            | Numérique    |
| `annual_distance`        | Distance annuelle déclarée                  | Numérique    |
| `commute_distance`       | Distance au lieu de travail                 | Numérique    |
| `conv_count_3_yrs_minor` | Nombre de contraventions                    | Numérique    |
| `veh_age`                | Âge du véhicule                             | Numérique    |
| `years_claim_free`       | Nombre d'années sans réclamation            | Numérique    |
| `years_licensed`         | Nombre d'année depuis l'obtention du permis | Numérique    |
| `distance`               | Distance réelle parcourue                   | Numérique    |
| `gender`                 | Genre                                       | Catégorielle |
| `marital_status`         | Statut marital                              | Catégorielle |
| `pmt_plan`               | Plan de paiement                            | Catégorielle |
| `veh_use`                | Utilisation du véhicule                     | Catégorielle |
]

- Ainsi que le **nombre de réclamations** pour chaque contrat.
---

# Introduction

## Données &amp;#8212; Télématique

| Contract ID | Trip ID | Departure datetime | Arrival datetime | Distance | Maximum speed |
|:-----------:|:------:|:-----------------:|:----------------:|:--------:|:-------------:|
| A | 1 | 2017-05-02 19:04:15 | 2017-05-02 19:24:24 | 25.0 | 104 |
| A | 2 | 2017-05-02 21:31:29 | 2017-05-02 21:31:29 | 6.4 | 66 |
| `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` |
| A | 2320 | 2018-04-30 21:17:22 | 2018-04-30 21:18:44 | 0.2 | 27 |
| B | 1 | 2017-03-26 11:46:07 | 2017-03-26 11:53:29 | 1.5 | 76 |
| B | 2 | 2017-03-26 15:18:23 | 2017-03-26 15:51:46 | 35.1 | 119 |
| `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` |
&lt;!-- | B | 1485 | 2018-03-23 20:07:08 | 2018-03-23 20:20:30 | 10.1 | 92 | --&gt;
&lt;!-- | C | 1 | 2017-11-20 08:14:34 | 2017-11-20 08:40:21 | 9.7 | 78 | --&gt;
&lt;!-- | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | --&gt;

- 118 millions de trajets
- Période: 2015-2018

---

# Chapitre 1 - How much telematics...

## Motivations

- On sait que les données télématiques peuvent aider à mieux tarifer.
- Données volumineuses qui peuvent être difficile et coûteuses à stocker.
- De plus, peuvent être longues à processer.
- D'où la question « Quelle quantité d'information télématique a-t-on besoin? ».

---

# Chapitre 1 - How much telematics...

## Extraction de variables télématiques

&lt;img src="images/extraction.png" width="100%" style="display: block; margin: auto;" /&gt;

- Variables extraites à partir de la **date-heure** de **départ** et d'**arrivée**, de la **distance** et de la **vitesse maximale** de chaque trajet.

---

# Chapitre 1 - How much telematics...

## Performance des modèles

&lt;img src="images/performance_1.png" width="100%" style="display: block; margin: auto;" /&gt;

- Tous les modèles utilisent les **variables de tarification traditionnelle** + les **variables télématiques extraites**.
- Variable réponse: indicatrice d'une réclamation (0 ou 1).

---

background-image: url(images/schema.png)
background-size: 70%
background-position: 95% 80%

.vspace-neg2[]
# Chapitre 1 - How much telematics...

## Jeux de données de classification

- Les k jeux de données &lt;br&gt; ont en commun les &lt;br&gt; variables de tarification &lt;br&gt; traditionnelles.
- Seule la quantité &lt;br&gt; d'information &lt;br&gt; télématique varie entre &lt;br&gt; les jeux de données.

---

background-image: url(images/results_1.png)
background-size: 65%
background-position: 100% 80%

# Chapitre 1 - How much telematics...

## Résultats

- Expérience également faite avec &lt;br&gt; le nombre de mois de données &lt;br&gt; télématiques (résultats &lt;br&gt; non-montrés ici).
- Les données télématiques &lt;br&gt; deviennent redondantes &lt;br&gt; après environ **4000 km** ou &lt;br&gt; **3 mois**.


---

# Chapitre 2 - Enhancing Claim Classification...
.vspace-neg2[]
## Introduction et motivations

.titre-bloc.bleu[
Hypothèse
]
.bloc.titre.bleu[
Le degré de routine et de péculiarité des trajets d'un assuré peut nous aider à estimer son risque.
]

.vspace-neg2[]
.pull-left[
**Routine**
___
Mesure à quel point le trajet d'un assuré est similaire (ou différent) par rapport aux autres trajets de .bleu-gras[cet assuré].
]

.pull-right[
**Péculiarité**
___
Mesure à quel point le trajet est similaire (ou différent) par rapport aux trajets des .bleu-gras[autres assurés].
]

.titre-bloc.rouge[
Aperçu
]
.bloc.titre.rouge[
- On calcule un **score** de **routine** et de **péculiarité** pour chaque trajet d'un assuré en utilisant des algorithmes de **détection d'anomalies**.
- On extrait ensuite des **covariables** de ces scores.
]

???

- Avec la technologie télématique, chaque contrat d'assurance peut être lié à un historique complet de conduite.
- Comment traduire de manière optimale les données de conduite en variables de tarification?
- La plupart des études créent des variables « à la main ». Biais humain et choix subjectifs.
- Approche non-supervisée basée sur des méthodes de détection d'anomalies.
- L'approche non-supervisée a l'avantage d'être plus interprétable.
- Automatisation l'extraction de variables télématiques.

---

background-image: url(images/flowchart_2.png)
background-size: 85%
background-position: 100% 250%

# Chapitre 2 - Enhancing Claim Classification...

## Aperçu

---

# Chapitre 2 - Enhancing Claim Classification...

## Algorithmes de détection d'anomalies

.panelset.sideways[
.panel[.panel-name[Méthode de Mahalanobis]
&lt;img src="images/maha_ex.png" width="100%" style="display: block; margin: auto;" /&gt;

]
.panel[.panel-name[Local Outlier Factor]
&lt;img src="images/lof_ex.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.panel[.panel-name[Isolation Forest]
&lt;img src="images/if_ex.png" width="100%" style="display: block; margin: auto;" /&gt;
]
]

---

background-image: url(images/lof_ex.png)
background-size: 70%
background-position: 90% 90%

# Chapitre 2 - Enhancing Claim Classification...

## Algorithmes de détection d'anomalies

- Méthode de Mahalanobis
- Local Outlier Factor (LOF)
- Isolation Forest (IF)

---

# Chapitre 2 - Enhancing Claim Classification...

## Attributs utilisés pour la détection d'anomalies

- Chaque trajet est caractérisé par 4 données: **date-heure d'arrivée**, **date-heure de départ**, **distance** et **vitesse maximale**.
- On dérive **8 attributs** à partir de ces données: 

&lt;img src="images/attributes.png" width="100%" style="display: block; margin: auto;" /&gt;

- Les algorithmes de détection d'anomalies seront appliqués sur ce nuage de points en 8 dimensions.

---



<style>.panelset{--panel-tab-active-foreground: #0051BA;--panel-tab-hover-foreground: #d22;--panel-tab-font-family: Roboto;}</style>

# Chapitre 2 - Enhancing Claim Classification...

## Calcul des scores de routine et de péculiarité

.panelset.sideways[
.panel[.panel-name[Routine]
&lt;img src="images/routine.png" width="80%" style="display: block; margin: auto;" /&gt;

]
.panel[.panel-name[Péculiarité]
&lt;img src="images/peculiarite.png" width="80%" style="display: block; margin: auto;" /&gt;
]
]

---

# Chapitre 2 - Enhancing Claim Classification...

- TRF: les 10 variables de tarification traditionelles.
- Le tableau montre l'.bleu-gras[amélioration] par rapport au .bleu-gras[modèle de base].

&lt;img src="images/res_chap2.png" width="100%" style="display: block; margin: auto;" /&gt;

**Modèle utilisé**: régression logistique avec pénalité elastic-net.

---

# Chapitre 3 - Telematics Combined Actuarial...

## Introduction et motivations
.vspace-neg2[]

.left-column[
&lt;img src="images/tele_icon.png" width="60%" style="display: block; margin: auto;" /&gt;

&lt;img src="images/brain_icon.png" width="65%" style="display: block; margin: auto;" /&gt;

&lt;img src="images/nn_icon.png" width="60%" style="display: block; margin: auto;" /&gt;
]

.right-column[
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
.ecriture-grise[Depuis plusieurs années, les assureurs utilisent les informations télématiques dans leurs modèles de tarification.]
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
.ecriture-grise[La plupart du temps, le jugement humain est utilisé pour en extraire des variables de tarification.]
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
.ecriture-grise[Et si on automatisait cette extraction à l'aide d'un réseau neuronal?]
]

???

- Insurers have been leveraging telematics data in their pricing models for several years now.
- Using telematics data in pricing model has many benefits:
  - More precise pure premium
  - Incentive for safer and less frequent driving (less accidents, congestion and pollution)
  - Substitute to sensible rating factors
  - Etc.
- Insurers will most of the time take the raw telematics data and try to extract features from it that they think are correlated with the claiming risk:
  - harsh acceleration/braking
  - % night driving
  - % driving in different speed buckets
  - Etc.
- This is a good approach, but it heavily relies on human judgement, with its flaws and biases.
- For instance, how do we define "night driving" or "harsh braking"?
- An alternative approach is to let the data speak more freely by training a model that automatically learns useful features from raw data.
- This is what we do in this project, by training a neural network directly on raw telematics data, wih minimal human intervention.
- Indeed, NNs are known for being good at extracting features from minimally processed data.
- More specifically, we consider a special architecture called "Combined Actuarial Neural Network", which we compare with a benchmark that uses handcrafted telematics features.

---

# Chapitre 3 - Telematics Combined Actuarial...
&lt;div class="neg-break"&gt;&lt;/div&gt;
## Approche « Combined Actuarial Neural Network » (CANN)
&lt;div class="neg-break"&gt;&lt;/div&gt;
.pull-left[
&lt;img src="images/CANN.png" width="90%" style="display: block; margin: auto;" /&gt;

.center[Développée par M. Wüthrich et M. Merz dans [cet article](https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID3320525_code769240.pdf?abstractid=3320525)
]].
&lt;/br&gt;
&lt;/br&gt;
&lt;/br&gt;
.ecriture-grise[Il s'agit d'un .bleu-gras[modèle paramétrique classique] (souvent un GLM) auquel un .bleu-gras[réseau neuronal] a été « attaché ».]

.ecriture-grise[L'objectif de la partie réseau neuronal est de .bleu-gras[capturer] tout .bleu-gras[signal] qui aurait pu être manqué par le GLM.]

.ecriture-grise[Les paramètres de la partie GLM sont généralement initialisés au .bleu-gras[maximum de vraisemblance], tandis que ceux de la partie réseau neuronal sont initialisés à .bleu-gras[zéro].]

???

- Here is the generic neural network (NN) architecture we use to model the number of claims.
- Developed by Mario V. Wüthrich and Michael Merz, it is called the "Combined Actuarial Neural Network" or CANN for short.
- CANN is a two-part neural network consisting of the GLM part and the network part.
- The GLM part functions like a regular GLM, while the network attached to it aims to capture signals that may have been missed by the GLM.
- A GLM is limited in its ability to capture interactions between predictors and can only approximate linear functions of predictors. The network part helps improve performance by complementing the GLM.
- Typically, the parameters of the GLM part are initialized using maximum likelihood, while the parameters of the network part are initialized at zero.
- This initialization allows the entire neural network to provide decent predictions from the start. During training, the network part is trained on the residuals of the GLM, which can be seen as a neural network boosting of the GLM
- The CANN architecture offers several advantages:
  - The GLM part allows for better interpretability.
  - CANN starts with decent predictions from the GLM, resulting in faster training.
  - It is a flexible approach. In this example, both parts are fed with the same inputs, but the network part can also accommodate other types of inputs. Neural networks are known to be effective with unstructured data such as telematics data and text data.
- Another interesting aspect of this approach is that it can be used for feature extraction, with the extracted features then used in a separate GLM. This would allow to leverage the strengths of both neural networks and GLMs: neural networks for complex function approximation and GLMs for their desirable properties.
- Additionally, the features created in the hidden layers can be visualized using dimensionality reduction techniques, potentially providing insights into telematics data.
- In summary, this architecture could be valuable in various supervised learning problems, particularly when dealing with unstructured data.
- In fact, the project consists of training this architecture with telematics information incorporated into the network part. To accomplish this, we consider three popular distribution specifications for count data: Poisson and negative binomial for cross-sectional data, and the Multivariate Negative Binomial for longitudinal data.

---

# Chapitre 3 - Telematics Combined Actuarial...
&lt;div style="margin-top: -30px;"&gt;&lt;/div&gt;

## Données télématiques en entrée au réseau

&lt;div style="margin-top: -10px;"&gt;&lt;/div&gt;
.ecriture-grise-petite[On veut que le réseau apprenne à partir des données brutes, mais on a besoin d'un prétraitement minimal:]

.bloc.bleu[
`\(\boldsymbol{h} = (h_1, h_2, \dots, h_{24})\)` où `\(h_i\)` est la fraction de conduite au cours de la `\(i^\text{e}\)` heure de la journée.

`\(\boldsymbol{p} = (p_1, p_2, \dots, p_7)\)` où `\(p_i\)` est la fraction de conduite au cours du `\(i^\text{e}\)` jour de la semaine.

`\(\boldsymbol{vmo} = (vmo_1, vmo_2, \dots, vmo_{14})\)` où `\(vmo_i\)` est la fraction des trajets dans le `\(i^\text{e}\)` intervalle de vitesse moyenne.

`\(\boldsymbol{vma} = (vma_1, vma_2, \dots, vma_{16})\)` où `\(vmo_i\)` est la fraction des trajets dans le `\(i^\text{e}\)` intervalle de vitesse maximale.

`\(\boldsymbol{d} = (d_1, d_2, \dots, d_{10})\)` où `\(d_i\)` est la fraction des trajets dans le `\(i^\text{e}\)` intervalle de distance.
]

.ecriture-grise-petite[On .bleu-gras[concatène] ensuite ces quatre vecteurs en un grand vecteur d'entrée de dimension 24 + 7 + 14 + 16 + 10 = 71, qui servira d'entrée à la partie MLP du modèle CANN :]

`\(\textbf{vecteur_telematique} = (\boldsymbol{h}, \boldsymbol{p}, \boldsymbol{vmo}, \boldsymbol{vma}, \boldsymbol{d})\)`

???

- From this telematics dataset, we therefore create 4 telematics vectors.
- The first one, h, is of dimension 24 (for the 24 hours of the day), and each elements is the fraction of driving in the correponding hour of the day. For instance, `\(h_1\)` is the fraction of driving for a given contract made between midnight and 1:00.
- The second one is similar, but instead records the fraction of driving in each day of the week.
- vmo records the fraction of trips made in different buckets of average speed. I made 10 kilometers per hour buckets, so for instance, vmo_1 is the fraction of trips made at an average speed between 0 and 10 kph. 
- vma is the same, but for maximum speed. 
- We then concatenate these 4 vectors into 1 big telematics vector, which summarises the driving habits of a given contract.
- These telematics inputs are represented by green circles.

---

# Chapitre 3 - Telematics Combined Actuarial...

---

# Conclusion


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
