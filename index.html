<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Soutenance de thèse - Francis Duval</title>
    <meta charset="utf-8" />
    <meta name="author" content="Francis Duval" />
    <meta name="date" content="2024-02-21" />
    <script src="libs/header-attrs-2.23/header-attrs.js"></script>
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <link rel="stylesheet" href="theme_cara.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title-slide
background-image: url(images/logo_chaire.jpg), url(images/background.jpg)
background-size: 28%, cover
background-position: 97% 97%, center

.titre-page-titre[Modélisation des sinistres en assurance automobile avec l'utilisation de données télématiques : Approches d'apprentissage automatique en classification et régression de comptage]
&lt;br /&gt;
.sous-titre-page-titre[Soutenance de thèse]
&lt;br /&gt;
***
&lt;br /&gt;
.sous-sous-titre-page-titre[.mon-style-bleu[par] Francis Duval &lt;br&gt; .mon-style-bleu[le] 26 février 2024]

???

- Bonjour tout le monde! Tout d'abord, merci d'être venus assister à ma soutenance de thèse.
- Ma thèse s'intitule « Modélisation des sinistres en assurance automobile avec l'utilisation de données télématiques : Approches d'apprentissage automatique en classification et régression de comptage ».

---

# Contenu
&lt;br&gt;
.left[
  .bleu-gros[1) Introduction] &lt;br&gt; &lt;br&gt; &lt;br&gt;
  .bleu-gros[2) Chapitre 1] &lt;br&gt; **How Much Telematics Information Do Insurers Need for Claim Classification?** &lt;br&gt; &lt;br&gt;
  .bleu-gros[3) Chapitre 2] &lt;br&gt; **Enhancing Claim Classification with Feature Extraction From Anomaly-Detection-Derived Routine and Peculiarity Profiles** &lt;br&gt; &lt;br&gt;
  .bleu-gros[4) Chapitre 3] &lt;br&gt; **Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data** &lt;br&gt; &lt;br&gt;
  .bleu-gros[5) Conclusion]
]

???

- Voici un aperçu du contenu de la présentation.
- Dans la Section 1, l'introduction, je vais vous présenter quelques principes de base qui ont servi de piliers dans ma recherche. Je vais aussi vous présenter les données sur lesquelles j'ai travaillé.
- C'est une thèse par articles, c'est-à-dire qu'elle consiste en 3 articles distincts. Aux Sections 2, 3 et 4, je vous présente chacun de ces 3 articles.
- Finalement, on conclut à la Section 5.

---

# Introduction

## Assurance basée sur l'usage

- Tarification en assurance automobile traditionellement faite avec un ensemble limité de **caractéristiques du risque**.
- Avec l'avènement de la technologie **télématique**, possibilité de collecter des données sur la conduite des assurés.

.center[
&lt;img src="images/sep_car.png" width="4%" style="display: block; margin: auto;" /&gt;
]
.titre-bloc.bleu[
De nombreux bénéfices!
]
.bloc.titre.bleu[
- Incitatif pour une conduite plus parcimonieuse et sécuritaire.
  - Amélioration de la sécurité des routes.
  - Réduction des gaz à effet de serre.
- Tarification plus précise et équitable basée sur le comportement de conduite réel de chaque individu.
- Substituts à des attributs considérés éthiquement sensibles tels que le genre.
]

???

- Ma thèse vise à améliorer la modélisation des sinistres en assurance automobile, plus précisément en assurance basée sur l'usage.
- L'assurance basée sur l'usage est un modèle d'assurance dans lequel les primes sont calculée selon l'utilisation réelle faite du véhicule.
- Pour ce faire, on collecte des données sur la conduite de l'assuré, qu'on appelle « données télématiques ».
- Ces données sont collectées à l'aide d'un dispositif installé dans le véhicule avec une application mobile.
- Ces données peuvent inclure la distance parcourue par l'assuré, les heures de conduite, la localisation, les freinages brusques, et bien d'autres encore.
- Traditionnellement, la tarification est faite en ne regardant pas l'utilisation du véhicule. On tarifie avec un ensemble de caratéristiques du risque comme l'âge et le genre de l'assuré, l'âge du véhicule, la région et plusieurs autres.

- Tarifer selon l'utilisation offre de nombreux avantages pour tout le monde. 
- Avant tout, ça constitue un incitatif à mieux et à moins conduire pour les assurés, ce qui permet d'améliorer la sécurité des routes et de diminuer la pollution. Donc on a une diminution de l'aléa moral, un effet qui est omniprésent en assurance.
- Pour un assureur, ça permet de mieux segmenter la prime, et donc d'aller chercher un avantage compétitif.
- Les données télématiques peuvent également servir de substitus à des attributs sensibles ou des attributs qui sont interdits d'utilisation par la loi. Par exemple, dans certaines juridictions, on ne peut pas tarifer selon le genre de l'assuré.
- Je n'ai pas parlé des inconvénients ici, mais c'est certain qu'utilisées de manière intelligente et responsable, les bénéfices qu'on peut tirer des données télématiques surpassent les inconvénients, et c'est pour ça que c'est important de faire de la recherche là-dessus.   

---

background-image: url(images/machine_learning_picto.png)
background-size: 10%
background-position: 95% 20%

`$$\DeclareMathOperator*{\argmin}{argmin}$$`
.vspace-neg[]

# Introduction

## Apprentissage supervisé &amp;#x2014; Idée générale

.vspace-neg2[]

- Requiert un ensemble d'**exemples étiquetés**
`\begin{align*}
  \{(\boldsymbol{x}_i, y_i)\}_{i=1}^n, \quad \text{où}\quad \boldsymbol{x}_i \in \mathcal{X},\quad y \in \mathcal{Y}
\end{align*}`

???

- Le principal outil utilisé dans la thèse est l'apprentissage supervisé.
- Pour être entrainé, un algorithme d'apprentissage supervisé requiert un jeu de données d'exemples étiquetés.
- Donc pour chaque exemple i, on a un vecteur de prédicteurs x qui est associé à une étiquette, ou variable réponse y.
- Par exemple, la variable d'intérêt y pourrait être le coût de réclamation et x, un vecteur de caractéristiques du risque.

--

- But: modéliser la **variable réponse** `\(y\)` à l'aide des **prédicteurs** `\(\boldsymbol{x}\)`. On cherche une fonction `\(h: \mathcal{X} \rightarrow \mathcal{A}\)`, où `\(\mathcal{A}\)` est l'ensemble des prédictions possibles, telle que `\(h(\boldsymbol{x})\)` est la plus « proche » possible de `\(y\)`.

???

- y, c'est notre variable d'intérêt qu'on veut modéliser avec l'aide de notre vecteur x.
- Donc en fait, on cherche une fonction de x qui va nous donner une valeur la plus proche possible de y.

--

- Pour définir « proche », on introduit une **fonction de perte** `\(\ell: \mathcal{A} \times \mathcal{Y} \rightarrow \mathbb{R}^+\)`.
- `\(\ell[h(\boldsymbol{x}), y]\)` mesure la « distance » entre la prédiction `\(h(\boldsymbol{x})\)` et la réponse `\(y\)`.

???

- Pour définir « proche », on introduit une fonction de perte `\(\ell\)` qui va pour but de mesurer la « distance » entre la prédiction h(x) et la réponse y.

--

- Pour trouver une bonne fonction `\(h^*\)`, on va (essayer de) minimiser le **risque empirique**, c'est-à-dire la perte moyenne sur l'ensemble d'exemples:
`\begin{align*}
  h^* = \argmin_{h} \frac{1}{n}\sum_{i=1}^n \ell[h(\boldsymbol{x}_i), y_i]. 
\end{align*}`

???

- Et ensuite, pour trouver notre fonction h, on va habituellement essayer de minimiser ce qu'on appelle le « risque empirique » sur nos exemples d'entrainement.
- Le risque empirique, c'est simplement la somme (ou la moyenne) de la fonction de perte évaluée sur chacun des exemples.

--

- Plusieurs algorithmes d'apprentissage supervisés ont été développés pour résoudre ce problème d'optimisation (GLMs, arbres de décision, forêts aléatoires, boosting, réseaux de neurones, régularisation, etc.).

???

- Pour trouver une bonne fonction h, plusieurs algorithmes ont été développés.
- Ces algorithmes d'apprentissage nous définissent en fait un espace de recherche pour la fonction h ainsi qu'une recette pour trouver la fonction optimale (ou une bonne fonction) dans cet espace.

---

# Introduction

## Apprentissage supervisé &amp;#8212; Compromis biais-variance

**Décomposition de l'erreur**
`\begin{align*}
  \text{Erreur} = \text{Biais}^2 + \text{Variance} + \text{Erreur irréductible}
\end{align*}`

&lt;img src="images/biais_variance.png" width="90%" style="display: block; margin: auto;" /&gt;

- Le compromis biais-variance dit que l'on peut « troquer » du biais contre de la variance, et vice-versa.

???

- Un concept très important en apprentissage supervisé est le compromis biais-variance, qui est intimement lié aux concepts de sous-ajustement et surajustement.
- Donc tout d'abord, on peut décomposer l'erreur en 3 parties: le biais au carré, la variance et l'erreur irréductible.
- L'erreur irréductible, c'est l'erreur qu'on ferait même si on avait le meilleur modèle possible, c'est-à-dire si on savait la fonction qui a généré les données. On a pas de contrôle là-dessus.
- Par contre, on a le contrôle sur le biais et la variance, et donc si on veut diminuer l'erreur, on peut diminuer un des 2.
- Et ce que le compromis biais-variance dit en fait, c'est que si l'un augmente, alors l'autre va systématiquement diminuer, et vice-versa.
- L'idée en apprentissage supervisé est de trouver un bon compromis, c'est-à-dire un compromis qui va minimiser l'erreur.

- Donc ici à gauche, on a un modèle qui a une variance faible mais un biais élevé. On dit qu'il y a sous-ajustement. On aurait intérêt à augmenter la variance pour diminuer le biais. Et bien sûr, on espère que la diminution en biais va plus que compenser pour l'augmentation de la variance.
- À droite, on a un modèle qui a un biais faible mais une variance élevée On dit qu'il y a surajustement. On aurait intérêt à augmenter le biais pour diminuer la variance.
- Au milieu, c'est ce qu'on recherche, c'est-à-dire un bon compromis entre biais et variance.

---

# Introduction

## Apprentissage supervisé &amp;#8212; Calibration des hyperparamètres

- Paramètres qui **ne** sont **pas** optimisés directement par l'algorithme.
- On essaie habituellement plusieurs combinaisons d'hyperparamètres et on prend celle menant à la meilleure performance **hors-échantillon**.
  - Besoin d'un **ensemble de validation** ou de **validation croisée**.
  
&lt;img src="images/tuning.png" width="75%" style="display: block; margin: auto;" /&gt;

???

- Souvent, en apprentissage supervisé, on doit faire la calibration des hyperparamètres.
- Un hyperparamètre, c'est simplement un paramètre qui n'est pas entrainé (ou optimisé) directement par l'algorithme.
- Ces hyperparamètres doivent donc être optimisés par le modélisateur.
- Il existe plusieurs algorithmes pour faire la calibration des hyperparamètres, comme l'optimisation Bayesienne, mais une manière simple de faire (et souvent suffisante) est de se faire une grille et d'essayer toutes les combinaisons de cette grille. On choisit alors la combinaison menant à la meilleure performance sur des exemples hors-échantillon.
- Ici on a un exemple de recherche par grille pour un modèle de forêt aléatoire. Donc on a essayé 3 valeurs pour chacun des 2 hyperparamètres, et il semble que 3 variables candidates et un arbre de profondeur 5 ou 15 mène à la meilleure performance. 

---

# Introduction

## Apprentissage supervisé &amp;#8212; Estimation de l'erreur de généralisation

- L'erreur calculée sur l'ensemble de validation (ou par validation croisée) **n**'est **pas** un bon estimé de l'erreur de généralisation.
  - En effet, lors du processus de calibration des hyperparamètres, il y a « **fuite d'information** » de l'ensemble de validation vers l'ensemble d'entrainement.
- Il est donc crucial de garder un ensemble test complètement indépendant.

&lt;img src="images/separation.png" width="55%" style="display: block; margin: auto;" /&gt;

???

- Après voir calibré les hyperparamètres en entrainé notre modèle en utilisant les meilleurs hyperparamètres trouvés, on veut estimer sa performance de généralisation, c'est-à-dire la performance sur de toutes nouvelles données.
- La performance obtenues sur l'ensemble de validation ou par validation croisée n'est souvent pas une bonne estimation de la performance de généralisation.
- La raison est qu'il peut y avoir fuite d'information de l'ensemble de validation vers l'ensemble d'entrainement, ce qui va artificiellement augmenter la performance.
- Ça peut arriver par exemple si notre prétraitement (par exemple, centrer-réduire les prédicteurs) ne fait pas partie de notre pipeline de modélisation. Autrement dit, si on applique le prétraitement une seule fois sur le jeu de données au complet et qu'ensuite, on fait notre modélisation.
- Heureusement, des outils existent pour « greffer » un prétraitement des données au modèles. Par exemple, le pacakge « recipes » dans tidymodels ou bien les pipelines de sickit-learn en Python.
- Il y a aussi des manières plus subtiles de fuiter de l'information. Par exemple, le processus de calibration des hyperparamètres va presque systématiquement faire fuiter de l'information de l'ensemble de validation vers l'ensemble d'entrainement. Il y a en effet un genre de sélection artificielle des modèles qui se produit, et la performance de validation sera souvent gonflée (donc le modèle sera moins bon qu'on le pense).
- Il est donc recommandé, pour bien estimer la performance, de se garder un ensemble test qu'on utilise seulement qu'à la toute fin du processus, et seulement pour quelques modèles.

---

.vspace-neg2[]

# Introduction

## Données &amp;#8212; Variables de tarification traditionnelles

.my-one-page-font[
| Nom                      | Description                                 | Type         |
|--------------------------|---------------------------------------------|--------------|
| `expo`                   | Durée du contrat                            | Numérique    |
| `annual_distance`        | Distance annuelle déclarée                  | Numérique    |
| `commute_distance`       | Distance au lieu de travail                 | Numérique    |
| `conv_count_3_yrs_minor` | Nombre de contraventions                    | Numérique    |
| `veh_age`                | Âge du véhicule                             | Numérique    |
| `years_claim_free`       | Nombre d'années sans réclamation            | Numérique    |
| `years_licensed`         | Nombre d'année depuis l'obtention du permis | Numérique    |
| `distance`               | Distance réelle parcourue                   | Numérique    |
| `gender`                 | Genre                                       | Catégorielle |
| `marital_status`         | Statut marital                              | Catégorielle |
| `pmt_plan`               | Plan de paiement                            | Catégorielle |
| `veh_use`                | Utilisation du véhicule                     | Catégorielle |
]

- Ainsi que le **nombre de réclamations** pour chaque contrat.

???

- Maintenant, je vous présente les données utilisées. Ce sont des données gracieusement fournies par Co-operators via la Chaire.
- Ça c'est les facteurs de risque traditionnels que j'utilise, incluant l'exposition au risque.
- Ce sont des données par contrat, et pour chaque contrat, on a l'information du nombre de réclamations faites durant la période de couverture.

---

# Introduction

## Données &amp;#8212; Télématique

| Contract ID | Trip ID | Departure datetime | Arrival datetime | Distance | Maximum speed |
|:-----------:|:------:|:-----------------:|:----------------:|:--------:|:-------------:|
| A | 1 | 2017-05-02 19:04:15 | 2017-05-02 19:24:24 | 25.0 | 104 |
| A | 2 | 2017-05-02 21:31:29 | 2017-05-02 21:31:29 | 6.4 | 66 |
| `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` |
| A | 2320 | 2018-04-30 21:17:22 | 2018-04-30 21:18:44 | 0.2 | 27 |
| B | 1 | 2017-03-26 11:46:07 | 2017-03-26 11:53:29 | 1.5 | 76 |
| B | 2 | 2017-03-26 15:18:23 | 2017-03-26 15:51:46 | 35.1 | 119 |
| `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` |
&lt;!-- | B | 1485 | 2018-03-23 20:07:08 | 2018-03-23 20:20:30 | 10.1 | 92 | --&gt;
&lt;!-- | C | 1 | 2017-11-20 08:14:34 | 2017-11-20 08:40:21 | 9.7 | 78 | --&gt;
&lt;!-- | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | `\(\vdots\)` | --&gt;

- 118 millions de trajets
- Période: 2015-2018

???

- J'ai aussi accès à des données télématiques sous la forme de résumés de trajets.
- Chaque trajet est décrit par 4 attributs, soit la date et heure de départ et d'arrivée, la distance parcourue et la vitesse maximale atteinte.
- Évidemment, ces trajets peuvent être liés à la base de données par contrat.

---

# Chapitre 1 - How Much Telematics...

## Motivations

.titre-bloc.bleu[
Inconvénients
]
.bloc.titre.bleu[
- **Données télématiques volumineuses:**
  - Difficiles et coûteuses à stocker
  - Demandent beaucoup de puissance de calcul dans les algorithmes
]
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
D'où la question « .bleu-gras[Quelle quantité d'information télématique a-t-on besoin minimalement pour bien estimer le risque?] ».

???

- Ce qui nous amène au premier article, « How Much Telematics Information Do Insurers Need for Claim Classification? ».
- Donc la motivation pour cet article vient d'un inconvénient des données télématiques.
- Cet inconvénient, c'est que ce sont des données souvent très volumineuses.
- C'est donc difficile et coûteux à stocker, mais ce sont aussi des données qui coûtent cher en temps de calcul.
- On s'est donc posés la question à savoir quelle quantité d'information télématique est nécessaire pour avoir une bonne évaluation du risque.
- On tente de répondre à cette question sous l'angle de la classification supervisée.

---

# Chapitre 1 - How much telematics...

## Extraction de variables télématiques

&lt;img src="images/extraction.png" width="100%" style="display: block; margin: auto;" /&gt;

- Variables extraites à partir de la **date-heure** de **départ** et d'**arrivée**, de la **distance** et de la **vitesse maximale** de chaque trajet.

???

- On s'est tout d'abord créé des variables télématiques à partir des résumés de trajets. Donc les voici.
- Ce sont des variables qui, selon nous, pourraient être liées au risque de réclamer.
- Par exemple, on a la distance moyenne par jour et la proportion de conduite de nuit.

---

# Chapitre 1 - How much telematics...

## Performance des modèles

&lt;img src="images/performance_1.png" width="100%" style="display: block; margin: auto;" /&gt;

- Tous les modèles utilisent les **variables de tarification traditionnelle** + les **variables télématiques extraites**.
- Variable réponse: indicatrice d'une réclamation (0 ou 1).

???

- On a d'abord testé plusieurs modèles.
- Les GLM régularisés (donc le lasso et l'elastic-net) se sont avérés plus performants que la forêt aléatoire, ce qui suggère que la relation entre les prédicteurs et la réponse est plutôt linéaire.
- On a également testé le lasso et l'elastic-net avec toutes les interactions 2 à 2, ce qui n'a pas mené à un gain de performance, ce qui laisse penser encore une fois que la relation est plutôt linéaire.
- On choisit le lasso sans interactions, qui a la meilleure performance de validation croisée (à égalité avec l'elastic-net en fait, si on tient compte de l'écart-type entre parenthèses. On choisit le lasso puisqu'il mène à des modèles plus parcimonieux que l'elastic-net.).
- On a également estimé la performance de généralisation sur un ensemble test.

---

background-image: url(images/schema.png)
background-size: 70%
background-position: 95% 80%

.vspace-neg2[]
# Chapitre 1 - How much telematics...

## Jeux de données de classification

- Les k jeux de données &lt;br&gt; ont en commun les &lt;br&gt; variables de tarification &lt;br&gt; traditionnelles.
- Seule la quantité &lt;br&gt; d'information &lt;br&gt; télématique varie entre &lt;br&gt; les jeux de données.

???

- Donc avec notre modèle lasso en mains, voici comment on procède pour estimer la quantité de données télématiques nécessaire pour avoir une bonne estimation du risque.
- On se crée k jeux de données de classification. La différence entre les jeux de données est la quantité de données télématiques utilisée, qui va en augmentant de jeu de données en jeu de données.
- On va ensuite aller évaluer la performance de notre modèle de classification sur chaque jeu de données.

---

background-image: url(images/results_1.png)
background-size: 65%
background-position: 100% 80%

# Chapitre 1 - How much telematics...

## Résultats

- Expérience également faite avec &lt;br&gt; le nombre de mois de données &lt;br&gt; télématiques (résultats &lt;br&gt; non-montrés ici).
- Les données télématiques &lt;br&gt; deviennent redondantes &lt;br&gt; après environ **4000 km** ou &lt;br&gt; **3 mois**.

???

- Voici les résultats. Donc ici, chaque jeu de données a 1000km de données télématiques de plus que le précédent.
- On a utilisé du bootstrap pour obtenir une distribution de l'aire sous la courbe ROC pour chaque jeu de données.
- Il s'avère qu'après environ 4000km, la performance se stabilise. On a aussi le même graphique, mais où chaque jeu de données a un mois de données télématiques de plus que le précédent, et il se trouve que la performance se stabilise après environ 3 mois de données. 

---

# Chapitre 2 - Enhancing Claim Classification...
.vspace-neg2[]

## Introduction et motivations

.titre-bloc.bleu[
Hypothèse
]
.bloc.titre.bleu[
Le degré de routine et de péculiarité des trajets d'un assuré peut nous aider à estimer son risque.
]

.vspace-neg2[]
.pull-left[
**Routine**
___
Mesure à quel point le trajet d'un assuré est similaire (ou différent) par rapport aux autres trajets de .bleu-gras[cet assuré].
]

.pull-right[
**Péculiarité**
___
Mesure à quel point le trajet est similaire (ou différent) par rapport aux trajets des .bleu-gras[autres assurés].
]

.titre-bloc.rouge[
Aperçu
]
.bloc.titre.rouge[
- On calcule un **score** de **routine** et de **péculiarité** pour chaque trajet d'un assuré en utilisant des algorithmes de **détection d'anomalies**.
- On extrait ensuite des **covariables** à partir de ces scores.
]

???

- Ce qui nous amène au deuxième article « Enhancing Claim Classification with Feature Extraction From Anomaly-Detection-Derived Routine and Peculiarity Profiles ».
- Donc pour cet article, on a d'abord pensé que le degré de routine et/ou de péculiarité d'un conducteur assuré peut aider à mieux estimer son risque.

- Le degré de routine, ça répond vraiment à la question « Est-ce que le conducteur fait des trajets qui sont plutôt homogènes, ou plutôt hétérogènes? ». Donc un conducteur qui fait toujours les mêmes trajets aux mêmes heures est considéré routinier. Ça peut se calculer pour chaque conducteur de manière isolée des autres conducteurs.
- Le degré de péculiarité, ça répond plutôt à la question « Est-ce que le conducteur fait des trajets qui sont plutôt semblables aux trajets des autres conducteurs, ou plutôt similaires? ». Donc un conducteur qui fait souvent des trajets de 200km pendant la nuit serait probablement considéré péculier.

- Donc en résumé, dans cet article, on calcule un score de routine et un score de péculiarité pour chaque trajet en utilisant des algorithmes de détection d'anomalies non-supervisés.
- On extrait ensuite des variables à partir de ces scores, qu'on utilise dans des modèles de classification.

---

background-image: url(images/flowchart_2.png)
background-size: 85%
background-position: 100% 250%

# Chapitre 2 - Enhancing Claim Classification...

## Aperçu

???

- Voici un aperçu du projet.
- On a notre jeu de données télématiques ici, sur lequel on applique un algorithme de détection d'anomalies pour dériver un profil de routine et de péculiarité pour chaque conducteur.
- On va ensuite résumer ces profils, ce qui va constituer des variables qu'on pourra utiliser pour augmenter le jeu de données avec les variables traditionnelles.

---

# Chapitre 2 - Enhancing Claim Classification...

## Algorithmes de détection d'anomalies

.panelset.sideways[
.panel[.panel-name[Méthode de Mahalanobis]
&lt;img src="images/maha_ex.png" width="100%" style="display: block; margin: auto;" /&gt;

]
.panel[.panel-name[Local Outlier Factor]
&lt;img src="images/lof_ex.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.panel[.panel-name[Isolation Forest]
&lt;img src="images/if_ex.png" width="100%" style="display: block; margin: auto;" /&gt;
]
]

???

- On a testé 3 algorithmes de détection d'anomalies.
- Ce sont des algorithmes non-supervisés, donc qui utilisent seulement la matrice de design x. Ils n'utilisent pas la variable réponse y.
- Le but de ces algorithmes est d'assigner un score d'anomalie à chaque point du nuage de points. Plus le point est considéré anormal, plus son score est élevé.
- Dans ce projet, chaque trajet peut être considéré comme un point dans un nuage de points.
- C'est vraiment intéressant de savoir comment ces algorithmes là calculent les scores d'anomalies, mais je vais passer rapidement là-dessus.
- Une chose intéressante à remarquer est que le Local Outlier Factor est différent des 2 autres puisqu'il va aussi détecter les anomalies locales, et non seulement les anomalies globales.
- Les anomalies globales se retrouvent seulement au bord du nuage de points, tandis que les anomalies locales peuvent aussi être en plein milieu, mais à des endroits où il n'y a pas beaucoup de points.

---

# Chapitre 2 - Enhancing Claim Classification...

## Attributs utilisés pour la détection d'anomalies

- Chaque trajet est caractérisé par 4 données: **date-heure d'arrivée**, **date-heure de départ**, **distance** et **vitesse maximale**.
- On dérive **8 attributs** à partir de ces données: 

&lt;img src="images/attributes.png" width="100%" style="display: block; margin: auto;" /&gt;

- Les algorithmes de détection d'anomalies seront appliqués sur ce nuage de points en 8 dimensions.

???

- Ici, on a les 8 variables sur lesquelles on applique la détection d'anomalies.
- Pour les variables de l'heure de la journée et du temps de la semaine, on a fait un encodage sinus-cosinus pusique ce sont des varaibles cycliques.

---



<style>.panelset{--panel-tab-active-foreground: #0051BA;--panel-tab-hover-foreground: #d22;--panel-tab-font-family: Roboto;}</style>

# Chapitre 2 - Enhancing Claim Classification...

## Calcul des scores de routine et de péculiarité

.panelset.sideways[
.panel[.panel-name[Routine]
&lt;img src="images/routine.png" width="80%" style="display: block; margin: auto;" /&gt;

]
.panel[.panel-name[Péculiarité]
&lt;img src="images/peculiarite.png" width="80%" style="display: block; margin: auto;" /&gt;
]
]

???

- Donc pour calculer les scores de routine et de péculiarité, c'est simple, on applique la détection d'anomalies sur les trajets.
- La différence entre les 2, c'est que pour la routine, la détection d'anomalies est appliquée sur chaque conducteur (ou véhicule) en isolation. Pour la péculiarité, la détection d'anomalies est faite sur tous les trajets en même temps.
- Donc ici, le trajet va se voir assigner un haut score d'anomalie s'il est anormal par rapport aux autres trajets de ce véhicule.
- Tandis qu'ici, il va se voir assigner un haut score s'il est différent par rapport à tous les autres trajets du jeu de données.

---

# Chapitre 2 - Enhancing Claim Classification...

**A-t-on réussi à discriminer .bleu-gras[routiniers] et .bleu-gras[non-routiniers]?**

.panelset.sideways[
.panel[.panel-name[Un conducteur routinier] 
&lt;img src="images/routinier.png" width="100%" /&gt;
]
.panel[.panel-name[Un conducteur non-routinier]
&lt;img src="images/non-routinier.png" width="100%" /&gt;
]
]

???

- Est-ce qu'on a réussi à différencier les routiniers des non-routiniers avec la détection d'anomalies?
- Pour avoir une idée, j'ai d'abord trouvé un conducteur le plus routinier possible et un autre le plus  non-routinier dans la base de données.
- Donc celui-ci est routinier puisque ses trajets ont tendance à être faits au mêmes heures et toujours la même distance.
- Tandis que celui-ci est non-routiner.
- J'ai ensuite comparé la distribution des scores de routine pour chacun.

---

# Chapitre 2 - Enhancing Claim Classification...

**A-t-on réussi à discriminer .bleu-gras[routiniers] et .bleu-gras[non-routiniers]?**

&lt;img src="images/profile_routine_vs_nonroutine.png" width="100%" /&gt;

Distribution des scores de routine pour les 2 conducteurs.

???

- Malheureusement, il semble que nos scores aient de la difficulté à différencier entre les 2.

---

# Chapitre 2 - Enhancing Claim Classification...

**A-t-on réussi à discriminer .bleu-gras[péculiers] et .bleu-gras[non-péculiers]?**

.panelset.sideways[
.panel[.panel-name[Un conducteur péculier]
&lt;img src="images/peculier.png" width="100%" /&gt;
]
.panel[.panel-name[Un conducteur non-péculier]
&lt;img src="images/non-peculier.png" width="100%" /&gt;
]
]

Ligne noire: distribution globale

???

- Est-ce qu'on a réussi à différencier les péculiers des non-péculiers avec la détection d'anomalies?
- Donc encore une fois, j'ai trouvé un conducteur le plus péculier possible et un autre le plus non-péculier possible dans la base de données.
- Donc celui-ci est péculier puisque la distribution de ses trajets est différente de la distribution globale.
- Tandis que celui-ci est plutôt non-péculier.

---

# Chapitre 2 - Enhancing Claim Classification...

**A-t-on réussi à discriminer .bleu-gras[péculiers] et .bleu-gras[non-péculiers]?**

&lt;img src="images/profile_peculiar_vs_nonpeculiar.png" width="100%" /&gt;

Distribution des scores de péculiarité pour les 2 conducteurs.

???

- Ici, 2 des 3 scores ont réussi à bien différencier les 2.
- Et c'est dans le bon sens. En effet, le conducteur péculier a des scores plus élevés que le non-péculier.

---

# Chapitre 2 - Enhancing Claim Classification...

- TRF: les 10 variables de tarification traditionelles.
- Le tableau montre l'.bleu-gras[amélioration] par rapport au .bleu-gras[modèle de base].

&lt;img src="images/res_chap2.png" width="100%" style="display: block; margin: auto;" /&gt;

**Modèle utilisé**: régression logistique avec pénalité elastic-net.

???

- Voici les résultats. On compare avec notre modèle de base qui utilise les 10 variables traditionelles en plus de la distance parcourue.
- Donc la manière dont on donne les scores d'anomalie aux modèles, c'est qu'on va extraire les 11 déciles de la distribution, et on calcule aussi toutes les interactions 2 à 2 entre les déciles.
- Malheureusement, les scores de routines ne permettent pas d'améliorer la tarification.
- Mais les scores de péculiarité, oui, et c'est avec l'algorithme de Mahalanobis qu'on arrive aux meilleurs résultats, avec une amélioration de 0.0184 de l'AUC.
- Par contre, si c'était à recommencer, j'aurais probablement aussi corrigé pour d'autres variables télématiques.
- Le score de péculiarité est quand même assez fortement corrélé à la durée et à la distance du trajet. Il semble donc que ce score ait réussi à différencier les conducteurs d'autoroute et les conducteurs de ville.
- Il aurait donc probablement fallu corriger pour la distance moyenne ou la durée par trajet, et pas seulement pour la distance totale.

---

# Chapitre 3 - Telematics Combined Actuarial...

## Introduction et motivations
.vspace-neg2[]

.left-column[
&lt;img src="images/tele_icon.png" width="60%" style="display: block; margin: auto;" /&gt;

&lt;img src="images/brain_icon.png" width="65%" style="display: block; margin: auto;" /&gt;

&lt;img src="images/nn_icon.png" width="60%" style="display: block; margin: auto;" /&gt;
]

.right-column[
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
.ecriture-grise[Depuis plusieurs années, les assureurs utilisent les informations télématiques dans leurs modèles de tarification.]
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
.ecriture-grise[La plupart du temps, le jugement humain est utilisé pour en extraire des variables de tarification.]
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
.ecriture-grise[Et si on automatisait cette extraction à l'aide d'un réseau neuronal?]
]

???

- Ce qui nous amène au troisième article, « Telematics combined actuarial neural networks for cross-sectional and longitudinal claim count data ».
- Une approche courante en assurance basée sur l'usage est de créer des variables « à la main » à partir des données télématiques brutes.
- Une approche alternative consiste à laisser les données s'exprimer plus librement en entraînant un modèle qui apprend automatiquement des représentations utiles à partir des données brutes. Cette approche peut améliorer la performance.
- C'est ce qu'on fait dans cet article: on utilise une architecture spéciale de réseaux de neurones qui permet de traiter les données télématiques brutes.

&lt;!-- - Insurers have been leveraging telematics data in their pricing models for several years now. --&gt;
&lt;!-- - Using telematics data in pricing model has many benefits: --&gt;
&lt;!--   - More precise pure premium --&gt;
&lt;!--   - Incentive for safer and less frequent driving (less accidents, congestion and pollution) --&gt;
&lt;!--   - Substitute to sensible rating factors --&gt;
&lt;!--   - Etc. --&gt;
&lt;!-- - Insurers will most of the time take the raw telematics data and try to extract features from it that they think are correlated with the claiming risk: --&gt;
&lt;!--   - harsh acceleration/braking --&gt;
&lt;!--   - % night driving --&gt;
&lt;!--   - % driving in different speed buckets --&gt;
&lt;!--   - Etc. --&gt;
&lt;!-- - This is a good approach, but it heavily relies on human judgement, with its flaws and biases. --&gt;
&lt;!-- - For instance, how do we define "night driving" or "harsh braking"? --&gt;
&lt;!-- - An alternative approach is to let the data speak more freely by training a model that automatically learns useful features from raw data. --&gt;
&lt;!-- - This is what we do in this project, by training a neural network directly on raw telematics data, wih minimal human intervention. --&gt;
&lt;!-- - Indeed, NNs are known for being good at extracting features from minimally processed data. --&gt;
&lt;!-- - More specifically, we consider a special architecture called "Combined Actuarial Neural Network", which we compare with a benchmark that uses handcrafted telematics features. --&gt;

---

# Chapitre 3 - Telematics Combined Actuarial...
&lt;div class="neg-break"&gt;&lt;/div&gt;
## Approche « Combined Actuarial Neural Network » (CANN)
&lt;div class="neg-break"&gt;&lt;/div&gt;
.pull-left[
&lt;img src="images/CANN.png" width="90%" style="display: block; margin: auto;" /&gt;

.center[Développée par M. Wüthrich et M. Merz dans [cet article](https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID3320525_code769240.pdf?abstractid=3320525)
]].
&lt;/br&gt;
&lt;/br&gt;
&lt;/br&gt;
.ecriture-grise[Il s'agit d'un .bleu-gras[modèle paramétrique classique] (souvent un GLM) auquel un .bleu-gras[réseau neuronal] a été « attaché ».]

.ecriture-grise[L'objectif de la partie réseau neuronal est de .bleu-gras[capturer] tout .bleu-gras[signal] qui aurait pu être manqué par le GLM.]

.ecriture-grise[Les paramètres de la partie GLM sont généralement initialisés au .bleu-gras[maximum de vraisemblance], tandis que ceux de la partie réseau neuronal sont initialisés à .bleu-gras[zéro].]

???

- Voici l'architecture sur laquelle on se base. Ça s'appelle l'architecture CANN.
- C'est un réseau de neurones en deux parties composé de la partie GLM et de la partie réseau de neurones.
- La partie GLM fonctionne comme un GLM ordinaire, tandis que le réseau qui lui est attaché vise à capturer le signal qui auraient pu être manqué par le GLM.

- Un GLM est limité dans sa capacité à capturer les interactions entre les prédicteurs et ne peut qu'approximer des fonctions linéaires des prédicteurs. La partie réseau aide à améliorer les performances en complémentant le GLM.

- Généralement, les paramètres de la partie GLM sont initialisés en utilisant le maximum de vraisemblance, tandis que les paramètres de la partie réseau sont initialisés à zéro.
- Cette initialisation permet au réseau CANN de fournir des prédictions décentes dès le départ. Pendant l'entraînement, la partie réseau est entraînée sur les résidus du GLM, ce qui peut être vu comme un boosting du GLM par un réseau neurones.
- L'architecture CANN offre plusieurs avantages :
  - La partie GLM permet une meilleure interprétabilité.
  - CANN commence avec des prédictions décentes du GLM, ce qui permet un entraînement plus rapide.
  - C'est une approche flexible. Dans cet exemple, les deux parties sont alimentées avec les mêmes entrées, mais     la partie réseau peut également accueillir d'autres types d'entrées. Les réseaux neuronaux sont connus pour     être efficaces avec des données non structurées comme les données et les données texte. CE n'est pas            nécessairement obligé d'être un perceptron multicouches ici.

&lt;!-- - Here is the generic neural network (NN) architecture we use to model the number of claims. --&gt;
&lt;!-- - Developed by Mario V. Wüthrich and Michael Merz, it is called the "Combined Actuarial Neural Network" or CANN for short. --&gt;
&lt;!-- - CANN is a two-part neural network consisting of the GLM part and the network part. --&gt;
&lt;!-- - The GLM part functions like a regular GLM, while the network attached to it aims to capture signals that may have been missed by the GLM. --&gt;
&lt;!-- - A GLM is limited in its ability to capture interactions between predictors and can only approximate linear functions of predictors. The network part helps improve performance by complementing the GLM. --&gt;
&lt;!-- - Typically, the parameters of the GLM part are initialized using maximum likelihood, while the parameters of the network part are initialized at zero. --&gt;
&lt;!-- - This initialization allows the entire neural network to provide decent predictions from the start. During training, the network part is trained on the residuals of the GLM, which can be seen as a neural network boosting of the GLM --&gt;
&lt;!-- - The CANN architecture offers several advantages: --&gt;
&lt;!--   - The GLM part allows for better interpretability. --&gt;
&lt;!--   - CANN starts with decent predictions from the GLM, resulting in faster training. --&gt;
&lt;!--   - It is a flexible approach. In this example, both parts are fed with the same inputs, but the network part can also accommodate other types of inputs. Neural networks are known to be effective with unstructured data such as telematics data and text data. --&gt;
&lt;!-- - Another interesting aspect of this approach is that it can be used for feature extraction, with the extracted features then used in a separate GLM. This would allow to leverage the strengths of both neural networks and GLMs: neural networks for complex function approximation and GLMs for their desirable properties. --&gt;
&lt;!-- - Additionally, the features created in the hidden layers can be visualized using dimensionality reduction techniques, potentially providing insights into telematics data. --&gt;
&lt;!-- - In summary, this architecture could be valuable in various supervised learning problems, particularly when dealing with unstructured data. --&gt;
&lt;!-- - In fact, the project consists of training this architecture with telematics information incorporated into the network part. To accomplish this, we consider three popular distribution specifications for count data: Poisson and negative binomial for cross-sectional data, and the Multivariate Negative Binomial for longitudinal data. --&gt;

---

# Chapitre 3 - Telematics Combined Actuarial...
&lt;div style="margin-top: -30px;"&gt;&lt;/div&gt;

## Données télématiques en entrée au réseau

&lt;div style="margin-top: -10px;"&gt;&lt;/div&gt;
.ecriture-grise-petite[On veut que le réseau apprenne à partir des données brutes, mais on a besoin d'un prétraitement minimal:]

.bloc.bleu[
`\(\boldsymbol{h} = (h_1, h_2, \dots, h_{24})\)` où `\(h_i\)` est la fraction de conduite au cours de la `\(i^\text{e}\)` heure de la journée.

`\(\boldsymbol{p} = (p_1, p_2, \dots, p_7)\)` où `\(p_i\)` est la fraction de conduite au cours du `\(i^\text{e}\)` jour de la semaine.

`\(\boldsymbol{vmo} = (vmo_1, vmo_2, \dots, vmo_{14})\)` où `\(vmo_i\)` est la fraction des trajets dans le `\(i^\text{e}\)` intervalle de vitesse moyenne.

`\(\boldsymbol{vma} = (vma_1, vma_2, \dots, vma_{16})\)` où `\(vmo_i\)` est la fraction des trajets dans le `\(i^\text{e}\)` intervalle de vitesse maximale.

`\(\boldsymbol{d} = (d_1, d_2, \dots, d_{10})\)` où `\(d_i\)` est la fraction des trajets dans le `\(i^\text{e}\)` intervalle de distance.
]

.ecriture-grise-petite[On .bleu-gras[concatène] ensuite ces 5 vecteurs en un grand vecteur de dimension 24 + 7 + 14 + 16 + 10 = 71, qui servira d'entrée à la partie MLP du modèle CANN :]

`\(\textbf{vecteur_telematique} = (\boldsymbol{h}, \boldsymbol{p}, \boldsymbol{vmo}, \boldsymbol{vma}, \boldsymbol{d})\)`

???

- Donc ici j'explique comment on donne en entrée au réseau les données télématiques.
- On se créée 5 vecteurs, chacun représentant une information télématique.
- Donc le premier est pour l'heure de la journée du trajet, le deuxième pour le jour de la semaine, le troisième pour la vitesse moyenne, le quatrième pour la vitesse maximale et le cinquième pour la distance.
- On se créée des intervalles. Donc par exemple, la vitesse moyenne et la vitesse maximale sont par intervalles de 10km/h.
- On concatène ces 5 vecteurs en un seul, qui sera donné en entrée à la partie réseau du modèle CANN.
- Chaque conducteur a donc sa conduite décrite par ce vecteur télématique.

&lt;!-- - From this telematics dataset, we therefore create 4 telematics vectors. --&gt;
&lt;!-- - The first one, h, is of dimension 24 (for the 24 hours of the day), and each elements is the fraction of driving in the correponding hour of the day. For instance, `\(h_1\)` is the fraction of driving for a given contract made between midnight and 1:00. --&gt;
&lt;!-- - The second one is similar, but instead records the fraction of driving in each day of the week. --&gt;
&lt;!-- - vmo records the fraction of trips made in different buckets of average speed. I made 10 kilometers per hour buckets, so for instance, vmo_1 is the fraction of trips made at an average speed between 0 and 10 kph.  --&gt;
&lt;!-- - vma is the same, but for maximum speed.  --&gt;
&lt;!-- - We then concatenate these 4 vectors into 1 big telematics vector, which summarises the driving habits of a given contract. --&gt;
&lt;!-- - These telematics inputs are represented by green circles. --&gt;

---

# Chapitre 3 - Telematics Combined Actuarial...
.vspace-neg2[]
## Binomiale négative

.panelset.sideways[
.panel[.panel-name[PMF]
&lt;img src="images/eq_nb.png" width="100%" /&gt;
]
.panel[.panel-name[Architecture]
.vspace-neg[]
&lt;img src="images/architecture_nb.png" width="95%" /&gt;
]
]

???

- Donc on arrive au modèle.
- On a considéré en fait 3 distributions pour la variable réponse du nombre de réclamations: la Poisson, la binomiale négative et la binomiale négative multivariée, qui est pour les données longitudinales.
- On va passer la Poisson, puisque c'est un cas particulier de la binomiale négative.

- Donc pour la binomiale négative, voici la fonction de masse de probabilités.
- Cette spécification là a l'avantage par rapport à la Poisson d'admettre de la surdispersion grâce au paramètre phi, et va donc souvent mieux modéliser les réclamations que la Poisson.
- On a 2 paramètres à estimer: le paramètre mu et le paramètre phi.

- L'architecture ressemble à ça. Donc le réseau a 2 sorties puisqu'il y a 2 paramètres à modéliser.
- Tout d'abord, on a décidé de ne pas mettre d'hétérogénéité dans le paramètre phi, donc on va simplement mettre un paramètre entrainable qui va être passé dans la fonction softplus, pour s'assurer que phi est positif.
- Ensuite, on a que le paramètre mu est la somme des 2 parties du CANN (partie GLM et partie réseau).
- Les entrées ici sont le vecteur télématique en plus de variables de tarification traditionnelles. Donc on permet au réseau de considérer les interations entre la télématiques et les variables traditionnelles.
- Ces entrées passent par 3 couches pleinement connectées pour obtenir une valeur `\(a_1^4\)`.
- Pour la partie GLM, on met seulement les variables traditionnelles, donc pas de télématique.
- Pour obtenir le paramètre mu, on somme les 2 parties du réseau, et on applique la fonction softplus pour s'assurer que mu est positif.
- Donc les bêtas ici sont initialisés au maximum de vraisemblance, tandis que les poids et biais de la partie réseau sont initialisés près de zéro.
- La fonction de perte utilisées est la log-vraisemblance négative, et on peut entrainer facilement ce réseau en appliquant la descente de gradient.

---

# Chapitre 3 - Telematics Combined Actuarial...
.vspace-neg2[]
## Binomiale négative multivariée

.panelset.sideways[
.panel[.panel-name[PMF]
Sachant les réclamations passées, Y suit une **binomiale négative**:
&lt;br&gt;
&lt;br&gt;
&lt;img src="images/eq_mvnb.png" width="100%" /&gt;
&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
où
&lt;br&gt;
`\begin{align*}
  \alpha_{it} &amp;= \phi + \sum_{i'=1}^{t-1}y_{it'} \quad \text{(paramètre phi + somme réclamations passées)}\\
  \gamma_{it} &amp;= \phi + \sum_{i'=1}^{t-1}\mu(x_{it'}) \quad \text{(paramètre phi + somme mus passées)}
\end{align*}`
]
.panel[.panel-name[Architecture]
&lt;img src="images/architecture_mvnb.png" width="75%" /&gt;
]
]

???

- Donc ici, c'est notre modèle longitudinal, donc on utilise la spécification négative binomiale multivariée.
- On utilise le fait que sachant les réclamations passées d'un assuré, le nombre de réclamation Y_it suit une binomiale négative, mais où les paramètres alpha et gamma dépendent de la somme de réclamations passées et de la somme des mus estimés passés.
- Donc alpha_it est égal à un paramètre phi plus la somme des réclamations passées, et gamma_it est égal à phi plus la somme des mus passés.

- Voici l'architecture utilisées. On a 3 paramètres à modéliser, donc on a un réseau à 3 sorties.
- La partie du haut pour modéliser mu reste la même que pour la binomiale négative.
- Pour obtenir phi, même chose aussi, on a un paramètre entrainable qu'on passe dans la softplus.
- Ensuite pour obtenir alpha, on ajoute la somme des réclamations passées à phi.
- Pour obtenir gamma, on ajoute la somme des mus passées à phi.
- C'est au niveau de l'entrainement du réseau que ça se complique un peu.
- Pour le alpha, c'est facile, on a juste à se rajouter une colonne du nombre de réclamations passées dans notre jeu de données.
- C'est pour le gamma que c'est plus compliqué, parce que ça dépend de la somme des mus estimés passés, mais les mus estimés passés, c'est justement ce qu'on essaie de trouver avec ce modèles. On a un problème du type « la poule ou l'oeuf ».
- Ce qu'on fait alors, c'est qu'on se créée une colonne des mus passés, qu'on initialise avec des valeurs trouvées par un modèle classique.
- À chaque époque complétées dans l'entrainement du réseau, on va aller mettre à jour cette colonne avec les poids courants du modèle.

---

# Chapitre 3 - Telematics Combined Actuarial...

## Résultats
.vspace-neg2[]
% d'amélioration par rapport au modèle de base (modèle Poisson homogène).
&lt;img src="images/results_article_3.png" width="72%" style="display: block; margin: auto;" /&gt;

???

- Voici les résultats. Donc les pourcentages correspondent à l'amélioration par rapport au modèle de base, qui est un modèle Poisson homogène.
- On peut se concentrer sur la deuxième colonne, qui est avec données télématiques.
- On a nos 3 modèles CANN (Poisson, binomiale négative et binomiale négative multivariée) qu'on compare avec des modèles log-linéaires, ou des GLM si vous voulez.
- Ces modèles log-linéaires utilisent les données télématiques sous forme de variables crée à la main.
- On voit qu'il y a amélioration de la performance par rapport aux modèle log-linéaires, ce qui veut dire que le réseau CANN a réussi à se créer des représentations utiles à partir des données brutes.
- Aussi, le modèle longitudinal, donc le MVNB, surpasse les 2 autres modèles transversaux, ce qui suggère que même après avoir pris en compte les variables traditionnelles et télématiques, il reste encore de la dépendance entre les contrats, qui peut être convenablement prise en compte par un modèle longitudinal.
- Ici, on a choisit la MVNB mais en fait, n'importe quelle distribution multivariée peut être utilisée, en autant qu'on puisse l'exprimer comme une distribution univariée sachant les contrats passées.

---

# Conclusion

## &lt;span style="text-decoration: underline;"&gt;En résumé&lt;/span&gt;

### Chapitre 1

- Développement d'une méthode permettant de connaitre la quantité minimale d'information télématique nécessaire pour avoir une bonne estimation du risque.

### Chapitre 2

- Élaboration d'une méthode basée sur des algorithmes de détection d'anomalies pour extraire des profiles de routine et de péculiarité des conducteurs, et utilisation dans un modèle de classification.

### Chapitre 3

- Construction d'un modèle de comptage des réclamations basé sur l'architecture CANN et adapté à des données longitudinales.

---

# Remerciements

.pull-left[
**Mes directeurs de recherche**
- Jean-Philippe Boucher
- Mathieu Pigeon

**Mes parents**
- Simon Duval
- Brigitte Jalbert

**Aide avec le package Torch**
- Marc Morin

]
.pull-right[
**Les 3 membres du jury**
- Arthur Charpentier
- Frédéric Godin
- Marie-Hélène Descary

**Le représentant du doyen**
- Alain Desgagné
]
&lt;br&gt;
&lt;br&gt;
**Mes collègues de la Chaire CARA**

**La Chaire Co-operators en analyse des risques actuariels**

**Conseil de recherches en sciences naturelles et en génie du Canada**

**Les fonds de recherche du Québec**
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
